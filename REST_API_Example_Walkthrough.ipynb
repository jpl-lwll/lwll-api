{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is meant to serve as an example walk through of the REST API.\n",
    "\n",
    "For demonstration purposes, we use the secret token we use to run tests. YOU SHOULD USE YOUR OWN TEAMS SECRET.\n",
    "\n",
    "If you do not have your `Team Secret Key`, send a slack message to `Alice Yepremyan` or email to `alice.r.yepremyan@jpl.nasa.gov`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secret = os.environ.get('TEST_SECRET')\n",
    "# url = 'http://localhost:5000'\n",
    "url = 'https://api-dev.lollllz.com'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List all of the available problems\n",
    "\n",
    "### Note that for your development purposes this is a different list than what will be exposed during eval time. During eval time, you will be expected to go through all of the problems this returns, either sequentially or in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: We pass the environment variable `GOVTEAM_SECRET` through here in the headers. During prototyping of TA1 systems this will be empty and not matter, during evaluation, the govteam will inject this enviornment variable into your containers and is REQUIRED for hitting the eval endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "headers = {'user_secret': secret, 'govteam_secret': os.environ.get('GOVTEAM_SECRET')}\n",
    "r = requests.get(f\"{url}/list_tasks\", headers=headers)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*NOTE* \n",
    "There are two tasks, `problem_test_image_classification`, `problem_test_obj_detection` that list their problem types in the name. These are given for demonstration and testing purposes in this notebook. We also include them in the `dev`, `staging`, and `eval` endpoints to run unit tests against, but they are not true evaluation tasks. True evaluation tasks are named with a uuid as in the other development tasks you see listed. You can find the tasks' problem types listed in their metadata.\n",
    "For an *evaluation*, you should *filter out* these two testing problem types as shown below, but do not hard code the number of tasks your program runs as these are not guaranteed to be in order. \n",
    "(It will not count against you if you simply run all of the tasks, but you may not wish to waste compute time on these tasks.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = [task for task in r.json()['tasks'] if task not in ['problem_test_image_classification',\n",
    "  'problem_test_obj_detection', 'problem_test_video_classification']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get A Task's Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "headers = {'user_secret': secret, 'govteam_secret': os.environ.get('GOVTEAM_SECRET')}\n",
    "r = requests.get(f\"{url}/task_metadata/problem_test_image_classification\", headers=headers)\n",
    "print(json.dumps(r.json(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also get metadata on each whitelisted dataset like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.json()['task_metadata']['whitelist']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`domain_net-painting` is in the whitelist, so we will get its metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'user_secret': secret, 'govteam_secret': os.environ.get('GOVTEAM_SECRET')}\n",
    "r = requests.get(f\"{url}/dataset_metadata/domain_net-painting\", headers=headers)\n",
    "print(json.dumps(r.json(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will get all of the tasks of each type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to get all tasks of a particular type\n",
    "from typing import List\n",
    "def get_task_subset_by_type(subset_type: str, url: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Helper function that returns the task ids in a list that match a specified\n",
    "    problem type\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    \n",
    "    subset_type : str\n",
    "        The task_type subset you want to get back\n",
    "    \"\"\"\n",
    "    headers = {'user_secret': secret, 'govteam_secret': os.environ.get('GOVTEAM_SECRET')}\n",
    "    tasks = requests.get(f\"{url}/list_tasks\", headers=headers)\n",
    "    task_list = tasks.json()['tasks']\n",
    "    subset_tasks = []\n",
    "    for _task in task_list:\n",
    "        r = requests.get(f\"{url}/task_metadata/{_task}\", headers=headers)\n",
    "        task_metadata = r.json()\n",
    "        try:\n",
    "            if task_metadata['task_metadata']['problem_type'] == subset_type:\n",
    "                subset_tasks.append(_task)\n",
    "        except Exception as e:\n",
    "            print(_task)\n",
    "            print(e)\n",
    "    return subset_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_classification_tasks = get_task_subset_by_type('image_classification', url)\n",
    "obj_detection_tasks = get_task_subset_by_type('object_detection', url)\n",
    "machine_translation_tasks = get_task_subset_by_type('machine_translation', url)\n",
    "video_classification_tasks = get_task_subset_by_type('video_classification', url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_classification_tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to filter out `problem_test_image_classification` in the evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_detection_tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to filter out `problem_test_obj_detection` in the evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_translation_tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to filter out `problem_test_video_classification` in the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_classification_tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the Task Training and Test Data\n",
    "Now that you see what the task dataset names are, you can use the download.py script from the [dataset_prep repo](https://gitlab.lollllz.com/lwll/dataset_prep). \n",
    "\n",
    "\n",
    "In this example walkthrough, the first task we will be using is `problem_test_image_classification`, so we would download the base and adaptation datasets listed in the metadata. In this case both the base and adaptation in `mnist`, though that will usually not be the case. \n",
    "\n",
    "`python download.py --dataset mnist --stage development --output ~/lwll_datasets --overwrite True`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a New Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'user_secret': secret, 'govteam_secret': os.environ.get('GOVTEAM_SECRET')}\n",
    "\n",
    "# This is a convenience for development purposes, IN EVAL ALWAYS USE `full`\n",
    "data_type = 'sample' # can either be `sample` or `full`\n",
    "\n",
    "# Option to customize the session name \n",
    "r = requests.post(f\"{url}/auth/create_session\", json={'session_name': 'testing', 'data_type': data_type, \n",
    "                                                      'task_id': 'problem_test_image_classification'},\n",
    "                  headers=headers)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_token = r.json()['session_token']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View the Current Session's Metadata at any time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'user_secret': secret, 'session_token': session_token, 'govteam_secret': os.environ.get('GOVTEAM_SECRET')}\n",
    "\n",
    "r = requests.get(f\"{url}/session_status\", headers=headers)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(f\"{url}/get_seen_labels\", headers=headers)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View the List of Active Sessions at any time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This shows the active sessions for your team\n",
    "headers_session = {'user_secret': secret, 'govteam_secret': os.environ.get('GOVTEAM_SECRET')}\n",
    "\n",
    "r = requests.get(f\"{url}/list_active_sessions\", headers=headers_session)\n",
    "active_sessions = r.json()\n",
    "active_sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deactivate Sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows how to progromatticaly deactivate a session. We comment this out for now to show functionality across the rest of the workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's deactivate the last used session\n",
    "# deactivate_session = active_sessions['active_sessions'][-1]\n",
    "# headers_active_session = {'user_secret': secret, 'session_token': session_token, 'govteam_secret': os.environ.get('GOVTEAM_SECRET')}\n",
    "\n",
    "# r = requests.post(f\"{url}/deactivate_session\", json={'session_token': deactivate_session}, headers=headers_active_session)\n",
    "# print(r.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r = requests.get(f\"{url}/session_status\", headers=headers_active_session)\n",
    "# r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If len(active_sessions['active_sessions']) was 1, then run this cell to create a new session again.\n",
    "#Since we don't have any sessions active now, we need to create a new one\n",
    "# headers = {'user_secret': secret, 'govteam_secret': os.environ.get('GOVTEAM_SECRET')}\n",
    "\n",
    "# # This is a convenience for development purposes, IN EVAL ALWAYS USE `full`\n",
    "# data_type = 'sample' # can either be `sample` or `full`\n",
    "\n",
    "# # Option to customize the session name \n",
    "# r = requests.post(f\"{url}/auth/create_session\", json={'session_name': 'testing', 'data_type': data_type, 'task_id': tasks[0]}, headers=headers)\n",
    "# session_token = r.json()['session_token']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip Checkpoints\n",
    "Performers are able to skip checkpoints BUT you may NOT skip the last checkpoint in the base and adaptation stages respectively (ie checkpoint 8 and checkpoint 16). This will result in an exception ERROR. Moreover you should NOT skip more than 2 checkpoints. Below is an example for how to skip a checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'user_secret': secret, 'session_token': session_token, 'govteam_secret': os.environ.get('GOVTEAM_SECRET')}\n",
    "\n",
    "r = requests.get(f\"{url}/skip_checkpoint\", headers=headers)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Seed Labels\n",
    "Seed labels may be requested for the first 4 checkpoints, which have label budgets of 1, 2, 4, 8 per class. Note that the `secondary_seed_labels` endpoint has been removed, and you may instead call `seed_labels` repeatedly.\n",
    "\n",
    "*Note* that in the event you try calling `seed_labels` outside of the appropriate checkpoint, you will encounter an error. This route is only available for the call on the correct checkpoint stage and it will automatically reduce the `budget_left_until_checkpoint` variable. The tasks have been designed such that after calling the `seed_labels` route, you will be exactly at the correct number to submit predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "headers = {'user_secret': secret, 'session_token': session_token, 'govteam_secret': os.environ.get('GOVTEAM_SECRET')}\n",
    "\n",
    "r = requests.get(f\"{url}/seed_labels\", headers=headers)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our budget for the first checkpoint is one label per class, in this case 10, which we have reached. Let's take a look at the Session Metadata to see that we have our `budget_left_until_checkpoint` equal to 0 now. This means we must submit predictions for our holdout test set now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'user_secret': secret, 'session_token': session_token, 'govteam_secret': os.environ.get('GOVTEAM_SECRET')}\n",
    "\n",
    "r = requests.get(f\"{url}/session_status\", headers=headers)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submitting Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each dataset is budgeted into 8 checkpoints for base and adaptation. We have just completed the first stage of requesting labels, and we must submit predictions. In this notebook we will show the two supported methods for submitting predictions. Below we will simply generate random predictions to demonstrate how to submit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Tuple, List\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My path to my predownloaded datasets by using the download.py utility found \n",
    "# here: https://gitlab.lollllz.com/lwll/dataset_prep\n",
    "# DATASETS_PATH = Path.home().joinpath('lwll_datasets/development')\n",
    "DATASETS_PATH = Path.home() / \"Documents\" / \"LWLL\" / \"lwll_datasets\" / \"development\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_images_and_classes(dataset_path: Path, session_token: str, data_type: str='sample') -> Tuple[List[str],List[str]]:\n",
    "    \"\"\"\n",
    "    Helper method to dynamically get the test labels and give us the possible classes that can be submitted\n",
    "    for the current dataset\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    \n",
    "    dataset_path : Path\n",
    "        The path to the `development` dataset downloads\n",
    "    \n",
    "    session_token : str\n",
    "        Your current session token so that we can look up the current session metadata\n",
    "    \n",
    "    data_type: str\n",
    "        Indicates whether you are using the `sample` or `full` dataset. \n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    Tuple[List[str], List[str]]\n",
    "        The list of test image ids needed to submit a prediction and the list of class names that you can predict against\n",
    "    \"\"\"\n",
    "    # Then we can just reference our current metadata to get our dataset name and use that in the path\n",
    "    headers = {'user_secret': secret, 'session_token': session_token, 'govteam_secret': os.environ.get('GOVTEAM_SECRET')}\n",
    "    r = requests.get(f\"{url}/session_status\", headers=headers)\n",
    "    current_dataset = r.json()['Session_Status']['current_dataset']\n",
    "    current_dataset_name = current_dataset['name']\n",
    "    current_dataset_classes = current_dataset['classes']\n",
    "\n",
    "    test_imgs_dir = dataset_path.joinpath(f\"{current_dataset_name}/{current_dataset_name}_{data_type}/test\")\n",
    "    test_imgs = [f.name for f in test_imgs_dir.iterdir() if f.is_file()]\n",
    "    return test_imgs, current_dataset_classes\n",
    "\n",
    "def generate_random_predictions_on_test_set(test_imgs: List[str], current_dataset_classes: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generates a prediction dataframe for image classification based on random sampling from our available classes\n",
    "    \n",
    "    example:\n",
    "    \n",
    "    ========  =====\n",
    "    id        class\n",
    "    ========  =====\n",
    "    6831.png  '3'\n",
    "    1186.png  '9'\n",
    "    8149.png  '6'\n",
    "    4773.png  '3'\n",
    "    3752.png  '10'\n",
    "    ========  =====\n",
    "    \"\"\"\n",
    "    rand_lbls = [str(random.choice(current_dataset_classes)) for _ in range(len(test_imgs))]\n",
    "    df = pd.DataFrame({'id': test_imgs, 'class': rand_lbls})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_imgs, current_dataset_classes = get_test_images_and_classes(DATASETS_PATH, session_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use our looked up test image ids along with random sampling from the available classes to make our\n",
    "# prediction DataFrame\n",
    "df = generate_random_predictions_on_test_set(test_imgs, current_dataset_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As predictions submission requires multiple, computationally expensive metrics to be calculated, it can occassionally be slow to wait for a response. For this reason, there are two ways we can submit using the `{url}/submit_predictions?metrics` flag. The default way if you don't explicitly set this flag is `{url}/submit_predictions?metrics=true` and will return just the `accuracy` metric. If you change the flag to `{url}/submit_predictions?metrics=false` though, no metrics will be returned and the endpoint should return much faster. Regardless of how the flag is set though, on the last endpoint, all metrics will be returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'user_secret': secret, 'session_token': session_token, 'govteam_secret': os.environ.get('GOVTEAM_SECRET')}\n",
    "\n",
    "r = requests.post(f\"{url}/submit_predictions\", json={'predictions': df.to_dict()}, headers=headers)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example Response with f\"{url}/submit_predictions?metrics=true\"\n",
    "```\n",
    "{\n",
    "    'Session_Status': {\n",
    "        'accuracy': 0.123,\n",
    "        'active': 'In Progress',\n",
    "        'session_name': 'testing',\n",
    "        'session_token': 'token',\n",
    "        'task_id': 'task_id',\n",
    "        'uid': 'uid',\n",
    "        'user_name': 'name'\n",
    "    }\n",
    "}\n",
    "```\n",
    "##### Example Response with f\"{url}/submit_predictions?metrics=false\"\n",
    "```\n",
    "{\n",
    "    'Session_Status': {\n",
    "        'session_token': 'token'\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the eval1 version of the api, we could get a second round of seed labels from `secondary_seed_labels`. This endpoint has been removed, and we simply call `seed_labels` again. It will return an additional example from each class, so that at the second checkpoint we have a total of 2 examples per class. You could also choose to stop calling `seed_labels`, and instead query for 10 labels of your choosing. For this example, we will get a second set of seed labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'user_secret': secret, 'session_token': session_token, 'govteam_secret': os.environ.get('GOVTEAM_SECRET')}\n",
    "\n",
    "r = requests.get(f\"{url}/seed_labels\", headers=headers)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exactly as in the first checkpoint, if we inspect the Session Metadata, we will see our `budget_left_until_checkpoint` equal to 0 now. This means we must submit predictions for our holdout test set again for our second checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'user_secret': secret, 'session_token': session_token, 'govteam_secret': os.environ.get('GOVTEAM_SECRET')}\n",
    "\n",
    "r = requests.get(f\"{url}/session_status\", headers=headers)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_imgs, current_dataset_classes = get_test_images_and_classes(DATASETS_PATH, session_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = generate_random_predictions_on_test_set(test_imgs, current_dataset_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'user_secret': secret, 'session_token': session_token, 'govteam_secret': os.environ.get('GOVTEAM_SECRET')}\n",
    "\n",
    "r = requests.post(f\"{url}/submit_predictions\", json={'predictions': df.to_dict()}, headers=headers)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are at the third checkpoint. Once again, you can choose to call the `seed_labels` endpoint to be given an additional 2 examples per class or to query for labels. We will now request some labels by id, and then attempt to call `seed_labels` again to demonstrate a potential error. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Request Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can request labels up until we exhaust out `budget_left_until_checkpoint`. You can request labels by id at *any* of the checkpoints, including the first four if you have not already requested seed labels for that checkpoint. \n",
    "\n",
    "**Note: If you request less than the `budget_left_until_checkpoint` out of the dataset and then submit predictions, your session will look as if you requested all labels for that checkpoint. Because of this it is to your advantange to request as many labels as you have available to you or else you are going into future checkpoints with less information**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_labels_from_train_dataset(dataset_path: Path, session_token: str, n: int=None, data_type: str='sample') -> List[str]:\n",
    "    \"\"\"\n",
    "    Helper function to get a random `n` image ids from our train dataset to request labels for\n",
    "    from the api\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    \n",
    "    dataset_path : Path\n",
    "        The path to the `development` dataset downloads\n",
    "    \n",
    "    session_token : str\n",
    "        Your current session token so that we can look up the current session metadata\n",
    "    data_type: str\n",
    "        Indicates whether you are using the `sample` or `full` size dataset\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    List[str]\n",
    "        A list of n unique image ids for the current session dataset\n",
    "        \n",
    "    \"\"\"\n",
    "    headers = {'user_secret': secret, 'session_token': session_token, 'govteam_secret': os.environ.get('GOVTEAM_SECRET')}\n",
    "    r = requests.get(f\"{url}/session_status\", headers=headers)\n",
    "    current_dataset = r.json()['Session_Status']['current_dataset']\n",
    "    current_dataset_name = current_dataset['name']\n",
    "    budget_left = r.json()['Session_Status']['budget_left_until_checkpoint'] \n",
    "    if not n:\n",
    "        n = budget_left\n",
    "        print(f\"budget_left is {budget_left}\")\n",
    "        \n",
    "    train_imgs_dir = dataset_path.joinpath(f\"{current_dataset_name}/{current_dataset_name}_{data_type}/train\")\n",
    "    train_imgs = [f.name for f in train_imgs_dir.iterdir() if f.is_file()]\n",
    "    random_ids = random.sample(train_imgs, k=n)\n",
    "    return random_ids\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 20 labels left in our budget, but for illustration, we will first request 10. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_to_be_labeled = get_random_labels_from_train_dataset(DATASETS_PATH, session_token, n=10)\n",
    "images_to_be_labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "headers = {'user_secret': secret, 'session_token': session_token, 'govteam_secret': os.environ.get('GOVTEAM_SECRET')}\n",
    "\n",
    "query = {\n",
    "    'example_ids': images_to_be_labeled\n",
    "}\n",
    "\n",
    "r = requests.post(f\"{url}/query_labels\", json=query, headers=headers)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our `budget_left_until_checkpoint` is 10. Note that now that we have started to query for labels by id at this checkpoint, we cannot get `seed_labels` for the remainder of our budget, we must have the entire budget for the checkpoint available if we want to get `seed_labels`. If we call `seed_labels` again, we will get an error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'user_secret': secret, 'session_token': session_token, 'govteam_secret': os.environ.get('GOVTEAM_SECRET')}\n",
    "\n",
    "r = requests.get(f\"{url}/seed_labels\", headers=headers)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will query by id again until we run out of `budget_left_until_checkpoint`. Then we have to submit predictions to advance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_to_be_labeled = get_random_labels_from_train_dataset(DATASETS_PATH, session_token, n=10)\n",
    "images_to_be_labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'user_secret': secret, 'session_token': session_token, 'govteam_secret': os.environ.get('GOVTEAM_SECRET')}\n",
    "\n",
    "query = {\n",
    "    'example_ids': images_to_be_labeled\n",
    "}\n",
    "\n",
    "r = requests.post(f\"{url}/query_labels\", json=query, headers=headers)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have 0 labels left in our budget, and we must submit our predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = generate_random_predictions_on_test_set(test_imgs, current_dataset_classes)\n",
    "headers = {'user_secret': secret, 'session_token': session_token, 'govteam_secret': os.environ.get('GOVTEAM_SECRET')}\n",
    "\n",
    "r = requests.post(f\"{url}/submit_predictions\", json={'predictions': df.to_dict()}, headers=headers)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are on the fourth checkpoint, and we may request `seed_labels` again for an additional four examples per class. We will request seed labels again for demonstration, but it may not be to your advantage to request seed labels after querying by id because seed labels are chosen deterministically in advance for each task, so you may get the same label as one of the ones you requested by id. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'user_secret': secret, 'session_token': session_token, 'govteam_secret': os.environ.get('GOVTEAM_SECRET')}\n",
    "\n",
    "r = requests.get(f\"{url}/seed_labels\", headers=headers)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our session status will show that we have used all of our budget for this checkpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'user_secret': secret, 'session_token': session_token, 'govteam_secret': os.environ.get('GOVTEAM_SECRET')}\n",
    "\n",
    "r = requests.get(f\"{url}/session_status\", headers=headers)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit predictions\n",
    "df = generate_random_predictions_on_test_set(test_imgs, current_dataset_classes)\n",
    "headers = {'user_secret': secret, 'session_token': session_token, 'govteam_secret': os.environ.get('GOVTEAM_SECRET')}\n",
    "\n",
    "r = requests.post(f\"{url}/submit_predictions\", json={'predictions': df.to_dict()}, headers=headers)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are at the fifth checkpoint. The 5th through 8th checkpoint budget labels are on a logarithmic scale from the fourth checkpoint size to the total dataset size. Note that if you try to call `seed_labels` outside of the first four checkpoints, you will get an error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'user_secret': secret, 'session_token': session_token, 'govteam_secret': os.environ.get('GOVTEAM_SECRET')}\n",
    "\n",
    "r = requests.get(f\"{url}/seed_labels\", headers=headers)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the 5th through 8th checkpoints, you may only query by id and submit. At this point we have 15 labels left in our budget for this checkpoint. If you query for less than the labels in the budget and submit, you will forgo the additional labels you could have received and the `budget_used` will automatically by increased to the budget of the checkpoint for which you submitted. To demonstrate, we will request 10 labels and then submit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "images_to_be_labeled = get_random_labels_from_train_dataset(DATASETS_PATH, session_token, n=10)\n",
    "images_to_be_labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'user_secret': secret, 'session_token': session_token, 'govteam_secret': os.environ.get('GOVTEAM_SECRET')}\n",
    "\n",
    "query = {\n",
    "    'example_ids': images_to_be_labeled\n",
    "}\n",
    "\n",
    "r = requests.post(f\"{url}/query_labels\", json=query, headers=headers)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our `budget_left` is 5 and `budget_used` is 90, but if we submit now, our `budget_used` will automatically be increased to the budget of the checkpoint: `95`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit predictions\n",
    "df = generate_random_predictions_on_test_set(test_imgs, current_dataset_classes)\n",
    "headers = {'user_secret': secret, 'session_token': session_token, 'govteam_secret': os.environ.get('GOVTEAM_SECRET')}\n",
    "\n",
    "r = requests.post(f\"{url}/submit_predictions\", json={'predictions': df.to_dict()}, headers=headers)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilistic Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second way is to report the probabilities for each class by submitting a dataframe with the columns 'id' and a column for each class name. For instance, the following example is based on MNIST. The columns are the string names of the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_probabilities_on_test_set(test_imgs: List[str], current_dataset_classes: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generates a prediction dataframe for image classification with probabilities.\n",
    "    \n",
    "    example:\n",
    "    \n",
    "    ========  ====  ====  ====  ====  ====  ====  ====  ====  ====\n",
    "    id        '1'   '2'   '3'   '4'   '5'   '6'   '7'   '8'   '9'\n",
    "    ========  ====  ====  ====  ====  ====  ====  ====  ====  ====\n",
    "    6831.png  0.01  0.09  0.0   0.25  0.65  0.0   0.0   0.0   0.0\n",
    "    1186.png  0.15  0.0   0.20  0.25  0.05  0.35  0.0   0.0   0.0\n",
    "    8149.png  0.80  0.10  0.0   0.05  0.0   0.05  0.0   0.0   0.0\n",
    "    4773.png  0.0   0.7   0.0   0.15  0.15  0.0   0.0   0.0   0.0\n",
    "    3752.png  0.0   0.10  0.0   0.0   0.0   0.9   0.0   0.0   0.0\n",
    "    ========  ====  ====  ====  ====  ====  ====  ====  ====  ==== \n",
    "    \"\"\"\n",
    "    probabilities = []\n",
    "    for _ in range(len(test_imgs)):\n",
    "        a = np.random.random(size=len(current_dataset_classes))\n",
    "        a /= a.sum()\n",
    "        probabilities.append(a)\n",
    "    df = pd.DataFrame(probabilities, columns=current_dataset_classes)\n",
    "    df['id'] = test_imgs\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = generate_random_probabilities_on_test_set(test_imgs, current_dataset_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'user_secret': secret, 'session_token': session_token, 'govteam_secret': os.environ.get('GOVTEAM_SECRET')}\n",
    "\n",
    "r = requests.post(f\"{url}/submit_predictions\", json={'predictions': df.to_dict()}, headers=headers)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have gone through 5 of the 8 checkpoints at this point, let's just finish out some additional dummy submissions until we do our 8th submission, finishing out the `pair_stage` = `base` and so we can move on to the `adaptation` phasae. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in (range(2)):\n",
    "    # Checkpoint ith submission\n",
    "    r = requests.post(f\"{url}/submit_predictions\", json={'predictions': df.to_dict()}, headers=headers)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We had 8 stages, we submitted 8 sets of predictions\n",
    "\n",
    "## Notice when we query for session_status now, our `pair_stage` = `adaptation` instead of `base`. Also, we have a different set of `current_label_budget_stages`\n",
    "\n",
    "Our example is reusing the `mnist` dataset, however, there will almost always be a different dataset between `base` and `adaptation`. This `mnist` -> `mnist` example is just showing how to use the api."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-shot Learning Workflow\n",
    "\n",
    "For zero-shot learning, we can only use `get_seen_labels` to request labels for training images that belong to the seen classes. Calls to the `seed_labels` endpoint will result in an exception.\n",
    "\n",
    "## Metrics\n",
    "The optional ZSL task returns the accuracy, average per-class recall, ROC_AUC, and top-5 accuracy. These metrics will be calculated separately for the unseen classes, seen classes, as well as the overall dataset with all classes. The `submit_predictions` endpoint will return the following metrics in `response[\"Session_Status\"][\"checkpoint_scores\"]`: \n",
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-color:#ccc;border-spacing:0;}\n",
    ".tg td{background-color:#fff;border-color:#ccc;border-style:solid;border-width:1px;color:#333;\n",
    "  font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;word-break:normal;}\n",
    ".tg th{background-color:#f0f0f0;border-color:#ccc;border-style:solid;border-width:1px;color:#333;\n",
    "  font-family:Arial, sans-serif;font-size:14px;font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n",
    ".tg .tg-0lax{text-align:left;vertical-align:top}\n",
    "</style>\n",
    "<table class=\"tg\">\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th class=\"tg-0lax\">Metric</th>\n",
    "    <th class=\"tg-0lax\">Class splits</th>\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td class=\"tg-0lax\" rowspan=\"3\">Accuracy (Top-1 and Top-5)</td>\n",
    "    <td class=\"tg-0lax\">All</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0lax\">Seen</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0lax\">Unseen</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0lax\" rowspan=\"3\">Average per-class recall</td>\n",
    "    <td class=\"tg-0lax\">All</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0lax\">Seen</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0lax\">Unseen</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0lax\" rowspan=\"3\">ROC_AUC</td>\n",
    "    <td class=\"tg-0lax\">All</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0lax\">Seen</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0lax\">Unseen</td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session status:\n",
      "{'ZSL': True,\n",
      " 'active': 'In Progress',\n",
      " 'budget_left_until_checkpoint': 0,\n",
      " 'budget_used': 0,\n",
      " 'checkpoint_scores': [],\n",
      " 'current_dataset': {'classes': ['forest',\n",
      "                                 'buildings',\n",
      "                                 'river',\n",
      "                                 'mobile_home_park',\n",
      "                                 'harbor',\n",
      "                                 'golf_course',\n",
      "                                 'agricultural',\n",
      "                                 'runway',\n",
      "                                 'baseball_diamond',\n",
      "                                 'overpass',\n",
      "                                 'chaparral',\n",
      "                                 'tennis_court',\n",
      "                                 'intersection',\n",
      "                                 'airplane',\n",
      "                                 'parking_lot',\n",
      "                                 'sparse_residential',\n",
      "                                 'medium_residential',\n",
      "                                 'dense_residential',\n",
      "                                 'beach',\n",
      "                                 'freeway',\n",
      "                                 'storage_tanks'],\n",
      "                     'dataset_type': 'image_classification',\n",
      "                     'license_citation': 'Yang, Yi, and Shawn Newsam. \"Bag-of-visual-words and spatial extensions for land-use classification.\" In '\n",
      "                                         'Proceedings of the 18th SIGSPATIAL international conference on advances in geographic information systems, '\n",
      "                                         'pp. 270-279. 2010.',\n",
      "                     'license_link': 'None',\n",
      "                     'license_requirements': 'None',\n",
      "                     'name': 'UCMerced_LandUse',\n",
      "                     'number_of_channels': 3,\n",
      "                     'number_of_classes': 21,\n",
      "                     'number_of_samples_test': 500,\n",
      "                     'number_of_samples_train': 336,\n",
      "                     'seen_classes': ['airplane',\n",
      "                                      'baseball_diamond',\n",
      "                                      'beach',\n",
      "                                      'buildings',\n",
      "                                      'chaparral',\n",
      "                                      'forest',\n",
      "                                      'golf_course',\n",
      "                                      'harbor',\n",
      "                                      'intersection',\n",
      "                                      'medium_residential',\n",
      "                                      'overpass',\n",
      "                                      'parking_lot',\n",
      "                                      'runway',\n",
      "                                      'sparse_residential',\n",
      "                                      'storage_tanks',\n",
      "                                      'tennis_court'],\n",
      "                     'uid': 'UCMerced_LandUse',\n",
      "                     'unseen_classes': ['agricultural', 'dense_residential', 'freeway', 'mobile_home_park', 'river'],\n",
      "                     'zsl_description': {'airplane': 'A vehicle designed for air travel that is propelled through the air by one or more jet engines '\n",
      "                                                     'or propellers. From above, airplanes may appear as long, narrow objects with wings mounted on '\n",
      "                                                     'the top of the fuselage. They may be various colors and may have identifying markings such as '\n",
      "                                                     'logos or tail numbers.',\n",
      "                                         'baseball_diamond': 'A field where the game of baseball is played, consisting of a diamond-shaped infield '\n",
      "                                                             'with bases at each corner and an outfield. From above, a baseball diamond may appear '\n",
      "                                                             'as a small, oval-shaped field with distinctive markings on the grass and surrounding '\n",
      "                                                             'fences or walls.',\n",
      "                                         'beach': 'A strip of land along the edge of a body of water, typically a sea or ocean, that is covered by '\n",
      "                                                  'sand or small rocks. From above, a beach may appear as a wide, light-colored strip of land '\n",
      "                                                  'adjacent to a darker body of water, with a line of vegetation marking the border between the two. '\n",
      "                                                  'It may also feature structures such as boardwalks, piers, or beach houses.',\n",
      "                                         'storage_tanks': 'Large, cylindrical or rectangular containers used for storing liquids or gases, often '\n",
      "                                                          'made of metal or concrete. From above, storage tanks may appear as large, round or '\n",
      "                                                          'rectangular objects with a uniform shape and color, often found in industrial areas or '\n",
      "                                                          'near ports or terminals. They may be surrounded by other infrastructure such as '\n",
      "                                                          'pipelines, valves, or fencing.',\n",
      "                                         'tennis_court': 'A tennis court is a rectangular or oval-shaped area where the game of tennis is played. It '\n",
      "                                                         'is marked with a series of lines and features a net stretched across the center. From '\n",
      "                                                         'above, a tennis court may appear as a small, rectangular or oval-shaped area with '\n",
      "                                                         'distinctive white or green lines marking the playing surface. It may be surrounded by '\n",
      "                                                         'other features such as fencing, lights, or seating.'}},\n",
      " 'current_label_budget_stages': [],\n",
      " 'date_created': 1674164626000,\n",
      " 'date_last_interacted': 1674164626000,\n",
      " 'domain_adaptation_submitted': False,\n",
      " 'pair_stage': 'base',\n",
      " 'session_name': 'zsl_test_session',\n",
      " 'standard_zsl_scores': {},\n",
      " 'task_id': 'problem_test_zsl_2',\n",
      " 'uid': '6fcFtJBkuzXXXJhpf8db',\n",
      " 'user_name': 'JPL',\n",
      " 'using_sample_datasets': True}\n",
      "\n",
      "Session status after submission:\n",
      "=================================\n",
      "{'ZSL': True,\n",
      " 'active': 'Complete',\n",
      " 'budget_left_until_checkpoint': 0,\n",
      " 'budget_used': 0,\n",
      " 'checkpoint_scores': [{'accuracy_all_classes': 0.066,\n",
      "                        'accuracy_seen': 0.06611570247933884,\n",
      "                        'accuracy_unseen': 0.06569343065693431,\n",
      "                        'average_per_class_recall_all_classes': 0.06522620372875695,\n",
      "                        'average_per_class_recall_seen': 0.06562891788765693,\n",
      "                        'average_per_class_recall_unseen': 0.06393751842027703,\n",
      "                        'roc_auc_all_classes': 0.5092597253176943,\n",
      "                        'roc_auc_seen': None,\n",
      "                        'roc_auc_unseen': None,\n",
      "                        'top_5_accuracy_all_classes': 0.066,\n",
      "                        'top_5_accuracy_seen': 0.06611570247933884,\n",
      "                        'top_5_accuracy_unseen': 0.06569343065693431}],\n",
      " 'current_dataset': {'classes': ['forest', 'buildings', 'river',\n",
      "                                 'mobile_home_park', 'harbor', 'golf_course',\n",
      "                                 'agricultural', 'runway', 'baseball_diamond',\n",
      "                                 'overpass', 'chaparral', 'tennis_court',\n",
      "                                 'intersection', 'airplane', 'parking_lot',\n",
      "                                 'sparse_residential', 'medium_residential',\n",
      "                                 'dense_residential', 'beach', 'freeway',\n",
      "                                 'storage_tanks'],\n",
      "                     'dataset_type': 'image_classification',\n",
      "                     'license_citation': 'Yang, Yi, and Shawn Newsam. '\n",
      "                                         '\"Bag-of-visual-words and spatial '\n",
      "                                         'extensions for land-use '\n",
      "                                         'classification.\" In Proceedings of '\n",
      "                                         'the 18th SIGSPATIAL international '\n",
      "                                         'conference on advances in geographic '\n",
      "                                         'information systems, pp. 270-279. '\n",
      "                                         '2010.',\n",
      "                     'license_link': 'None',\n",
      "                     'license_requirements': 'None',\n",
      "                     'name': 'UCMerced_LandUse',\n",
      "                     'number_of_channels': 3,\n",
      "                     'number_of_classes': 21,\n",
      "                     'number_of_samples_test': 500,\n",
      "                     'number_of_samples_train': 336,\n",
      "                     'seen_classes': ['airplane', 'baseball_diamond', 'beach',\n",
      "                                      'buildings', 'chaparral', 'forest',\n",
      "                                      'golf_course', 'harbor', 'intersection',\n",
      "                                      'medium_residential', 'overpass',\n",
      "                                      'parking_lot', 'runway',\n",
      "                                      'sparse_residential', 'storage_tanks',\n",
      "                                      'tennis_court'],\n",
      "                     'uid': 'UCMerced_LandUse',\n",
      "                     'unseen_classes': ['agricultural', 'dense_residential',\n",
      "                                        'freeway', 'mobile_home_park',\n",
      "                                        'river']},\n",
      " 'current_label_budget_stages': [],\n",
      " 'date_created': 1674164626000,\n",
      " 'date_last_interacted': 1674164628308,\n",
      " 'domain_adaptation_score': {},\n",
      " 'domain_adaptation_submitted': False,\n",
      " 'pair_stage': 'base',\n",
      " 'session_name': 'zsl_test_session',\n",
      " 'standard_zsl_scores': {'accuracy_unseen_std': 0.19708029197080293,\n",
      "                         'average_per_class_recall_unseen_std': 0.19828902642695745,\n",
      "                         'roc_auc_unseen_std': None,\n",
      "                         'top_5_accuracy_unseen_std': 0.19708029197080293},\n",
      " 'task_id': 'problem_test_zsl_2',\n",
      " 'uid': '6fcFtJBkuzXXXJhpf8db',\n",
      " 'user_name': 'JPL',\n",
      " 'using_sample_datasets': True}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pprint import pformat\n",
    "\n",
    "url = 'https://api-dev.lollllz.com'\n",
    "\n",
    "# Base Headers\n",
    "headers = {\n",
    "    'user_secret': os.environ.get('TESTS_SECRET'),\n",
    "    'govteam_secret': os.environ.get('GOVTEAM_SECRET')\n",
    "}\n",
    "\n",
    "# Create session\n",
    "session_config = {\n",
    "    \"session_name\": \"zsl_test_session\", \"data_type\": \"sample\",\n",
    "    \"task_id\": \"problem_test_zsl_2\", \"ZSL\": True\n",
    "}\n",
    "session_token = requests.post(\n",
    "    f\"{url}/auth/create_session\", json=session_config, headers=headers\n",
    ").json()[\"session_token\"]\n",
    "\n",
    "session_headers = {\"session_token\": session_token, **headers}\n",
    "\n",
    "session_status = requests.get(\n",
    "    f\"{url}/session_status\",\n",
    "    headers=session_headers\n",
    ").json()[\"Session_Status\"]\n",
    "print(f\"Session status:\\n{pformat(session_status, depth=3, width=150)}\")\n",
    "\n",
    "# Add all labeled files to a list for use in training\n",
    "classes = session_status[\"current_dataset\"][\"classes\"]\n",
    "labeled_files = {class_: set() for class_ in classes}\n",
    "labeled_examples = requests.get(f\"{url}/get_seen_labels\", headers=session_headers).json()[\"Labels\"]\n",
    "for example in labeled_examples:\n",
    "    class_name, file = example[\"class\"], example[\"id\"]\n",
    "    labeled_files[class_name].add(file)\n",
    "\n",
    "# Obtain ZSL descriptions (optional)\n",
    "dataset_metadata = session_status[\"current_dataset\"]\n",
    "zsl_descriptions = dataset_metadata[\"zsl_description\"]\n",
    "unseen_classes = dataset_metadata.get(\n",
    "    \"unseen_classes\", sorted(zsl_descriptions.keys())\n",
    ")\n",
    "seen_classes = dataset_metadata.get(\n",
    "    \"seen_classes\", sorted(set(dataset_metadata[\"classes\"]) - set(unseen_classes))\n",
    ")\n",
    "\n",
    "# Train model using labeled_files (Example):\n",
    "# model = torch.hub.load(\"pytorch/vision:0.10.0\", \"resnet18\", pretrained=True)\n",
    "# train_loader = get_train_loader(labeled_files, batch_size=32)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=4e-3)\n",
    "# loss = torch.nn.CrossEntropyLoss()\n",
    "# train(model, num_epochs, train_loader, optimizer, loss)\n",
    "\n",
    "# To train model with TRAINING images from unseen classes, can do something like this:\n",
    "unseen_files = requests.get(f\"{url}/get_unseen_ids\", headers=session_headers).json()[\"ids\"]\n",
    "\n",
    "def model(input_files):  # dummy random model\n",
    "    return [str(random.choice(classes)) for _ in input_files]\n",
    "\n",
    "# Use trained model to obtain predictions\n",
    "## Obtain test files first\n",
    "data_root = os.path.join(\n",
    "    os.path.expanduser(\"~\"), \"Documents\", \"LWLL\", \"lwll_datasets\", \"development\"\n",
    ")\n",
    "dataset = session_status[\"current_dataset\"][\"name\"]\n",
    "test_folder = os.path.join(\n",
    "    data_root, dataset, f\"{dataset}_{session_config['data_type']}\", \"test\"\n",
    ")\n",
    "test_files = os.listdir(test_folder)\n",
    "\n",
    "# Optional: Train standard ZSL model and submit standard ZSL predictions\n",
    "def standard_zsl_model(input_files):  # dummy standard zsl model\n",
    "    return [str(random.choice(unseen_classes)) for _ in input_files]\n",
    "std_pred_df = pd.DataFrame({'id': test_files, 'class': standard_zsl_model(test_files)})\n",
    "## Submit standard ZSL prediction (predictions on images from seen classes are ignored)\n",
    "response = requests.post(\n",
    "    f\"{url}/submit_standard_zsl_predictions\",\n",
    "    json={\"predictions\": std_pred_df.to_dict()},\n",
    "    headers=session_headers,\n",
    ").json()\n",
    "\n",
    "# Submit generalized ZSL predictions\n",
    "preds = model(test_files)\n",
    "pred_df = pd.DataFrame({'id': test_files, 'class': preds})\n",
    "response = requests.post(\n",
    "    f\"{url}/submit_predictions\",\n",
    "    json={\"predictions\": pred_df.to_dict()},\n",
    "    headers=session_headers,\n",
    ").json()\n",
    "session_status_post_submission = response[\"Session_Status\"] \n",
    "# remove \"zsl_description\" from session status to avoid printing it\n",
    "import copy\n",
    "_s = copy.deepcopy(session_status_post_submission)\n",
    "_s[\"current_dataset\"].pop(\"zsl_description\")\n",
    "print(\n",
    "    f\"\\nSession status after submission:\\n\"\n",
    "    f\"=================================\\n\"\n",
    "    f\"{pformat(_s, compact=True)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Domain Adaptation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that now we have switched from the base dataset to the adaptation dataset. At this point, you may optionally submit predictions for unsupervised domain adaptation (UDA) before requesting any labels on the adaptation dataset using the endpoint `submit_UDA_predictions`.  There are two fields in the `task_metadata`, which we queried at the beginning of the notebook, that provide information about class overlap: `uda_base_to_adapt_overlap_ratio` and `uda_adapt_to_base_overlap_ratio`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.post(f\"{url}/submit_UDA_predictions\", json={'predictions': df.to_dict()}, headers=headers)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Gotchas and points to note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we completely finish our session, let's talk about some gotchas and requirements of these endpoints.\n",
    "\n",
    "**1.)** When you query for labels, if you ask for more labels than your `budget_left_until_checkpoint` in your session, it will be truncated and you will only recieve the number of labels up until that number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.)** The next gotcha is when we submit our predictions, YOU MUST submit all of the `id`'s and `label`'s. If you do not, you will encounter an error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_df = pd.DataFrame({'id':['14647.png', '33864.png'], 'class':['4', '3']})\n",
    "\n",
    "r = requests.post(f\"{url}/submit_predictions\", json={'predictions': bad_df.to_dict()}, headers=headers)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrap up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To finish out our `session`, let's just submit valid final predictions and then notice how on the last one, our `active` flag turns to `False`. This means we have successfully finished!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(8):\n",
    "    r = requests.post(f\"{url}/submit_predictions\", json={'predictions': df.to_dict()}, headers=headers)\n",
    "    print(json.dumps(r.json(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now hit all the checkpoints and finished the session\n",
    "You'll notice after we finish a session we now have access to see our metric scores at the end and our `active` flag transitions from `In Progress` to `Complete`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'user_secret': secret, 'session_token': session_token, 'govteam_secret': os.environ.get('GOVTEAM_SECRET')}\n",
    "r = requests.get(f\"{url}/session_status\", headers=headers)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point in the automated system, you should go on to the next problem you see from the `/list_tasks` endpoint and go through until you are finished."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We now do a walk through of the second example problem type\n",
    "\n",
    "The second problem type is the `object_detection` one. The flow is the same, except that you will notice that we have different label format and scoring metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'user_secret': secret, 'govteam_secret': os.environ.get('GOVTEAM_SECRET')}\n",
    "r = requests.get(f\"{url}/task_metadata/problem_test_obj_detection\", headers=headers)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'user_secret': secret, 'govteam_secret': os.environ.get('GOVTEAM_SECRET')}\n",
    "\n",
    "# This is a convenience for development purposes, IN EVAL ALWAYS USE `full`\n",
    "data_type = 'sample' # can either be `sample` or `full`\n",
    "\n",
    "r = requests.post(f\"{url}/auth/create_session\", json={'session_name': 'testing', 'data_type': data_type, \n",
    "                                                      'task_id': 'problem_test_obj_detection'}, \n",
    "                  headers=headers)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_token = r.json()['session_token']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exactly as before we need to follow the steps of:\n",
    "\n",
    "For EACH `base` and `adaptation` phase:\n",
    "\n",
    "For the first 4 checkpoints\n",
    "- Call `/seed_labels` *or* `/query_labels`\n",
    "- Submit predictions\n",
    "\n",
    "\n",
    "Then the last 4 checkpoints\n",
    "- Query for additional labels to `/query_labels`\n",
    "- Submit predictions\n",
    "\n",
    "For the sake of just closing out our session since this example is to demonstrate the  format for object detection problems, we will loop through the `seed_labels` and  predictions. First we show the format of predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_imgs, current_dataset_classes = get_test_images_and_classes(DATASETS_PATH, session_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_predictions_on_test_set_obj_detection(test_imgs: List[str], current_dataset_classes: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generates a prediction dataframe for image classification based on random sampling from our available classes\n",
    "    \"\"\"\n",
    "    \n",
    "    # We just use random labels for example. Our labels have to have a bounding box, confidence and class for object detection\n",
    "    # bounding boxes are defined as '<xmin>, <ymin>, <xmax>, <ymax>''\n",
    "    # This would be your inferences filling this DataFrame though.\n",
    "    rand_lbls = ['20, 20, 80, 80' for _ in range(len(test_imgs))]\n",
    "    conf = [0.95 for _ in range(len(test_imgs))]\n",
    "    classes = [current_dataset_classes[0] for _ in range(len(test_imgs))]\n",
    "    df = pd.DataFrame({'id': test_imgs, 'bbox': rand_lbls, 'confidence': conf, 'class': classes})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = generate_random_predictions_on_test_set_obj_detection(test_imgs, current_dataset_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We submit these as a dictionary in this format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'user_secret': secret, 'session_token': session_token, 'govteam_secret': os.environ.get('GOVTEAM_SECRET')}\n",
    "\n",
    "r = requests.get(f\"{url}/seed_labels\", headers=headers)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(4):\n",
    "    headers = {'user_secret': secret, 'session_token': session_token, 'govteam_secret': os.environ.get('GOVTEAM_SECRET')}\n",
    "\n",
    "    r = requests.get(f\"{url}/seed_labels\", headers=headers)\n",
    "    r = requests.post(f\"{url}/submit_predictions\", json={'predictions': df.to_dict()}, headers=headers)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the last 4 checkpoints, you can query by id, but we will just submit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'user_secret': secret, 'session_token': session_token, 'govteam_secret': os.environ.get('GOVTEAM_SECRET')}\n",
    "# Base dataset predictions\n",
    "for _ in range(4):\n",
    "    r = requests.post(f\"{url}/submit_predictions\", json={'predictions': df.to_dict()}, headers=headers)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the test images and random predictions for `adaptation` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_imgs, current_dataset_classes = get_test_images_and_classes(DATASETS_PATH, session_token)\n",
    "df = generate_random_predictions_on_test_set_obj_detection(test_imgs, current_dataset_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'user_secret': secret, 'session_token': session_token, 'govteam_secret': os.environ.get('GOVTEAM_SECRET')}\n",
    "# Adaptation dataset predictions\n",
    "for _ in range(8):\n",
    "    r = requests.post(f\"{url}/submit_predictions\", json={'predictions': df.to_dict()}, headers=headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now verify that our session is over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'user_secret': secret, 'session_token': session_token, 'govteam_secret': os.environ.get('GOVTEAM_SECRET')}\n",
    "\n",
    "r = requests.get(f\"{url}/session_status\", headers=headers)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in image classification and object detection, for the first four checkpoints you can choose to request seed labels or query by id. For the last four checkpoints, you can only query by id. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'user_secret': secret, 'govteam_secret': os.environ.get('GOVTEAM_SECRET')}\n",
    "\n",
    "# This is a convenience for development purposes, IN EVAL ALWAYS USE `full`\n",
    "data_type = 'sample' # can either be `sample` or `full`\n",
    "\n",
    "r = requests.post(f\"{url}/auth/create_session\", json={'session_name': 'testing', 'data_type': data_type, 'task_id': 'problem_test_video_classification'}, headers=headers)\n",
    "r.json()\n",
    "session_token = r.json()['session_token']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get our first set of seed labels and look at the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "headers = {'user_secret': secret, 'session_token': session_token, 'govteam_secret': os.environ.get('GOVTEAM_SECRET')}\n",
    "\n",
    "r = requests.get(f\"{url}/seed_labels\", headers=headers)\n",
    "print(json.dumps(r.json(), indent = 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For video classification, you are given the start and end frame number for the action. To get all of the frames for the action, simply enumerate between this range and add `.jpg`. Frames will always be provided in `.jpg` form. For example, to get all the frames associated with the first label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = r.json()[\"Labels\"]\n",
    "action_frames = [str(i)+'.jpg' for i in range(labels[0]['start_frame'], labels[0]['end_frame'] +1)]\n",
    "action_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'user_secret': secret, 'session_token': session_token, 'govteam_secret': os.environ.get('GOVTEAM_SECRET')}\n",
    "\n",
    "r = requests.get(f\"{url}/session_status\", headers=headers)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to submit predictions before we can query for seed labels again. Here we define some helper functions to retreive the video train data and test ids.\n",
    "\n",
    "You will notice that in the test set metadata, you are given the start and end frames for each segment corresponding to one action. These are given so that you have the boundaries on which to perform inference, as we do not consider determining these boundaries to be part of the image classification task. THIS DATA IS TO BE USED ONLY FOR INFERENCE. ANY USE OF THE TEST DATA FOR TRAINING WILL BE CONSIDERED CHEATING. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_images_and_classes_vid(dataset_path: Path, session_token: str, data_type: str='sample') -> Tuple[List[str],List[str]]:\n",
    "    \"\"\"\n",
    "    Helper method to dynamically get the test labels and give us the possible classes that can be submitted\n",
    "    for the current dataset\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    \n",
    "    dataset_path : Path\n",
    "        The path to the `development` dataset downloads\n",
    "    \n",
    "    session_token : str\n",
    "        Your current session token so that we can look up the current session metadata\n",
    "    \n",
    "    data_type: str\n",
    "        Indicates whether you are using the `sample` or `full` dataset. \n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    Tuple[List[str], List[str]]\n",
    "        The list of test image ids needed to submit a prediction and the list of class names that you can predict against\n",
    "    \"\"\"\n",
    "    # Then we can just reference our current metadata to get our dataset name and use that in the path\n",
    "    headers = {'user_secret': secret, 'session_token': session_token, 'govteam_secret': os.environ.get('GOVTEAM_SECRET')}\n",
    "    r = requests.get(f\"{url}/session_status\", headers=headers)\n",
    "    current_dataset = r.json()['Session_Status']['current_dataset']\n",
    "    current_dataset_name = current_dataset['name']\n",
    "    current_dataset_classes = current_dataset['classes']\n",
    "\n",
    "    test_meta = pd.read_feather(dataset_path.joinpath(f\"{current_dataset_name}/labels_{data_type}/meta_test.feather\"))\n",
    "    test_ids = test_meta['id'].tolist()\n",
    "    return test_ids, current_dataset_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids, current_dataset_classes = get_test_images_and_classes_vid(dataset_path=DATASETS_PATH, \n",
    "                                                                    session_token=session_token, \n",
    "                                                                    data_type=data_type)\n",
    "df = generate_random_predictions_on_test_set(test_ids, current_dataset_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit our predictions and check the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "headers = {'user_secret': secret, 'session_token': session_token, 'govteam_secret': os.environ.get('GOVTEAM_SECRET')}\n",
    "\n",
    "r = requests.post(f\"{url}/submit_predictions\", json={'predictions': df.to_dict()}, headers=headers)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will request seed labels and submit three more times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    print(i+1)\n",
    "    headers = {'user_secret': secret, 'session_token': session_token, 'govteam_secret': os.environ.get('GOVTEAM_SECRET')}\n",
    "    r = requests.get(f\"{url}/seed_labels\", headers=headers)\n",
    "    # get session metadata\n",
    "    r = requests.get(f\"{url}/session_status\", headers=headers)\n",
    "    budget_used = r.json()['Session_Status']['budget_used']\n",
    "    budget_left = r.json()['Session_Status']['budget_left_until_checkpoint']\n",
    "    print(f\"Got seed labels. Budget used: {budget_used} Budget left: {budget_left}\")\n",
    "    r = requests.post(f\"{url}/submit_predictions\", json={'predictions': df.to_dict()}, headers=headers)\n",
    "print(json.dumps(r.json(), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " r = requests.get(f\"{url}/session_status\", headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying for video labels\n",
    "For the last four checkpoints, we can query for labels by id for a video segment. The segments are provided in the `meta_train.feather` file when you download the dataset. Notice that our helper function is changed slightly to reflect this. Instead of iterating over all the frames in the `train` folder, we load the `meta_train.feather` file to look up the video segment `id`s. You can query up until you reach your label budget. The format of the response is the same as for seed labels. In the following example, we will query for two ids. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_labels_from_train_dataset_vid(dataset_path: Path, session_token: str, n: int=None, data_type: str='sample') -> List[str]:\n",
    "    \"\"\"\n",
    "    Helper function to get random `n` video segment ids from our train dataset to request labels for\n",
    "    from the api\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    \n",
    "    dataset_path : Path\n",
    "        The path to the `development` dataset downloads\n",
    "    \n",
    "    session_token : str\n",
    "        Your current session token so that we can look up the current session metadata\n",
    "    data_type: str\n",
    "        Indicates whether you are using the `sample` or `full` size dataset\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    List[str]\n",
    "        A list of n unique image ids for the current session dataset\n",
    "        \n",
    "    \"\"\"\n",
    "    headers = {'user_secret': secret, 'session_token': session_token, 'govteam_secret': os.environ.get('GOVTEAM_SECRET')}\n",
    "    r = requests.get(f\"{url}/session_status\", headers=headers)\n",
    "    current_dataset = r.json()['Session_Status']['current_dataset']\n",
    "    current_dataset_name = current_dataset['name']\n",
    "    budget_left = r.json()['Session_Status']['budget_left_until_checkpoint'] \n",
    "    if not n:\n",
    "        n = budget_left\n",
    "        \n",
    "        print(f\"budget_left is {budget_left}\")\n",
    "      \n",
    "    meta_train_path = dataset_path.joinpath(f\"{current_dataset_name}/labels_{data_type}/meta_train.feather\")\n",
    "    meta_train = pd.read_feather(meta_train_path)\n",
    "    random_ids = meta_train['id'].sample(n=n).tolist()\n",
    "    return random_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_to_be_labeled = get_random_labels_from_train_dataset_vid(DATASETS_PATH, session_token, n=77)\n",
    "query = {\n",
    "    'example_ids': segments_to_be_labeled\n",
    "}\n",
    "\n",
    "r = requests.post(f\"{url}/query_labels\", json=query, headers=headers)\n",
    "print(len(r.json()['Labels']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now submit for the last 4 checkpoints to advance to the adaptation stage, but you could continue to query for labels throughout these checkpoints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "headers = {'user_secret': secret, 'session_token': session_token, 'govteam_secret': os.environ.get('GOVTEAM_SECRET')}\n",
    "\n",
    "for _ in range(4):\n",
    "    r = requests.post(f\"{url}/submit_predictions\", json={'predictions': df.to_dict()}, headers=headers)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optionally submit predictions for unsupervised domain adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.post(f\"{url}/submit_UDA_predictions\", json={'predictions': df.to_dict()}, headers=headers)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of closing out the session, we will now loop through the adaptation stage. As in the other problem types, for the adaptation stage in the first four checkpoints you may get seed labels or query for image labels, and submit predictions for each checkpoint. In the last four checkpoints you may query for image labels and submit predictions for each checkpoint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "headers = {'user_secret': secret, 'session_token': session_token, 'govteam_secret': os.environ.get('GOVTEAM_SECRET')}\n",
    "# Adaptation dataset predictions\n",
    "for _ in range(8):\n",
    "    r = requests.post(f\"{url}/submit_predictions\", json={'predictions': df.to_dict()}, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "headers = {'user_secret': secret, 'govteam_secret': os.environ.get('GOVTEAM_SECRET')}\n",
    "r = requests.get(f\"{url}/task_metadata/06023f86-a66b-4b2c-8b8b-951f5edd0f22\", headers=headers)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'user_secret': secret, 'govteam_secret': os.environ.get('GOVTEAM_SECRET')}\n",
    "\n",
    "# This is a convenience for development purposes, IN EVAL ALWAYS USE `full`\n",
    "data_type = 'full' # can either be `sample` or `full`\n",
    "\n",
    "r = requests.post(f\"{url}/auth/create_session\", json={'session_name': 'testing', 'data_type': data_type, 'task_id': '06023f86-a66b-4b2c-8b8b-951f5edd0f22'}, headers=headers)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_token = r.json()['session_token']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no concept of seed labels in machine translation, we just go through the 8 checkpoints in an active learning workflow. Note that if you try to call the `seed_labels` endpoint, you will receive a warning and `None` will be returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_data_mt(dataset_path: Path, session_token: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Helper method to dynamically get the test labels and give us the possible classes that can be submitted\n",
    "    for the current dataset\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    \n",
    "    dataset_path : Path\n",
    "        The path to the `development` dataset downloads\n",
    "    \n",
    "    session_token : str\n",
    "        Your current session token so that we can look up the current session metadata\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    pd.DataFrame\n",
    "        The DataFrame on which you can make queries against\n",
    "    \"\"\"\n",
    "    # Then we can just reference our current metadata to get our dataset name and use that in the path\n",
    "    headers = {'user_secret': secret, 'session_token': session_token, 'govteam_secret': os.environ.get('GOVTEAM_SECRET')}\n",
    "    r = requests.get(f\"{url}/session_status\", headers=headers)\n",
    "    current_dataset = r.json()['Session_Status']['current_dataset']\n",
    "    current_dataset_name = current_dataset['name']\n",
    "\n",
    "    test_df = pd.read_feather(str(dataset_path.joinpath(f\"{current_dataset_name}/{current_dataset_name}_{data_type}/train_data.feather\")))\n",
    "    return test_df\n",
    "\n",
    "def get_test_data_mt(dataset_path: Path, session_token: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Helper method to dynamically get the test labels and give us the possible classes that can be submitted\n",
    "    for the current dataset\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    \n",
    "    dataset_path : Path\n",
    "        The path to the `development` dataset downloads\n",
    "    \n",
    "    session_token : str\n",
    "        Your current session token so that we can look up the current session metadata\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    pd.DataFrame\n",
    "        The DataFrame on which you must make predictions from a 'source' column\n",
    "    \"\"\"\n",
    "    # Then we can just reference our current metadata to get our dataset name and use that in the path\n",
    "    headers = {'user_secret': secret, 'session_token': session_token}\n",
    "    r = requests.get(f\"{url}/session_status\", headers=headers)\n",
    "    current_dataset = r.json()['Session_Status']['current_dataset']\n",
    "    current_dataset_name = current_dataset['name']\n",
    "    \n",
    "    _path = str(dataset_path.joinpath(f\"{current_dataset_name}/{current_dataset_name}_{data_type}/test_data.feather\"))\n",
    "    test_df = pd.read_feather(_path)\n",
    "    return test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = get_train_data_mt(DATASETS_PATH, session_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check our session status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'user_secret': secret, 'session_token': session_token}\n",
    "r = requests.get(f\"{url}/session_status\", headers=headers)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will demonstrate what will happen if we try to request over our label budget. Our budget is `5000` characters, so first we will request 50 labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "headers = {'user_secret': secret, 'session_token': session_token, 'govteam_secret': os.environ.get('GOVTEAM_SECRET')}\n",
    "\n",
    "query = {\n",
    "    'example_ids': [str(i) for i in range(50)]\n",
    "}\n",
    "\n",
    "r = requests.post(f\"{url}/query_labels\", json=query, headers=headers)\n",
    "print(len(r.json()['Labels']))\n",
    "#show the format of 1 label\n",
    "print(r.json()['Labels'][0])      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check our session status. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'user_secret': secret, 'session_token': session_token, 'govteam_secret': os.environ.get('GOVTEAM_SECRET')}\n",
    "r = requests.get(f\"{url}/session_status\", headers=headers)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our `budget_left` is `296` characters. Let's try requesting an additional 10 labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = {\n",
    "    'example_ids': [str(i) for i in range(50, 60)]\n",
    "}\n",
    "\n",
    "r = requests.post(f\"{url}/query_labels\", json=query, headers=headers)\n",
    "len(r.json()['Labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This put us over our label budget, so we got fewer than the 10 sentences that we requested. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'user_secret': secret, 'session_token': session_token, 'govteam_secret': os.environ.get('GOVTEAM_SECRET')}\n",
    "r = requests.get(f\"{url}/session_status\", headers=headers)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now make a prediction submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = get_test_data_mt(DATASETS_PATH, session_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_predictions_on_test_set_mt(test_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generates a prediction dataframe for machine translation with fake prediction data\n",
    "    \"\"\"\n",
    "    \n",
    "    # We make predictions and want a DataFrame with the columns\n",
    "    # 'id' and 'text'\n",
    "    pred = 'The quick brown fox jumps over the lazy dog'\n",
    "    pred_list = [pred for _ in range(len(test_df))]\n",
    "    df = pd.DataFrame({'id': test_df['id'].tolist(), 'text': pred_list})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = generate_random_predictions_on_test_set_mt(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'user_secret': secret, 'session_token': session_token, 'govteam_secret': os.environ.get('GOVTEAM_SECRET')}\n",
    "r = requests.post(f\"{url}/submit_predictions\", json={'predictions': pred_df.to_dict()}, headers=headers)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We successfully submitted a prediction file, let's go through the rest of the 7 checkpoints submitting our dummy prediction file until we switch over to adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(7):\n",
    "    r = requests.post(f\"{url}/submit_predictions\", json={'predictions': pred_df.to_dict()}, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we see the dataset has flipped over to the `adaptation` dataset, which is `ted_talks`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get our dummy test labels for this new dataset and finish out our session by submitting the last 8 dummy checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = get_test_data_mt(DATASETS_PATH, session_token)\n",
    "pred_df = generate_random_predictions_on_test_set_mt(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'user_secret': secret, 'session_token': session_token, 'govteam_secret': os.environ.get('GOVTEAM_SECRET')}\n",
    "\n",
    "query = {\n",
    "    'example_ids': [str(i) for i in range(5000)]\n",
    "}\n",
    "\n",
    "r = requests.post(f\"{url}/query_labels\", json=query, headers=headers)\n",
    "print(len(r.json()['Labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(8):\n",
    "    r = requests.post(f\"{url}/submit_predictions\", json={'predictions': pred_df.to_dict()}, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can see that we successfully finished the task!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Initially written - 11/15/19 MH*\n",
    "\n",
    "*Update for cleanliness - 11/21/19 MH*\n",
    "\n",
    "*Added Object Detection support - 12/16/19 MH*\n",
    "\n",
    "*Added Machine Translation support - 12/22/19 MH*\n",
    "\n",
    "*Updated predictions naming schema form `label` to `class` for `image_classification` and `label` to `bbox` for `object_detection` - 1/28/20 MH*\n",
    "\n",
    "*Changed `get_session_token` to `create_session`, performers can now customize the `session_name` - 2/10/20 AY*\n",
    "\n",
    "*Updated to use example ids from new mnist base/adaptation splits - 04/01/20 AD*\n",
    "\n",
    "*Removed machine translation, updated submission times for new numbers of label budgets - 04/29/20 AD*\n",
    "\n",
    "*Verified updated task metadata, included some automation around getting test images MH*\n",
    "\n",
    "*Added machine translation back in with the implementation of the secondary backend - 5/17/20 MH*\n",
    "\n",
    "*Added example of passing govteam_secret through request headers for eval time - 7/25/20 MH*\n",
    "\n",
    "*Removed `secondary_seed_labels` and updated `seed_labels` examples to reflect the eval2 budget scheme. `seed_labels` can now be called for the first four checkpoints. - 02/02/20 AD*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lwll_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13 (default, Mar 29 2022, 02:18:16) \n[GCC 7.5.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "dd6171ead6565875cada86ed73e8f0a1656c2c35f355d2862b8ffb78a320201f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
